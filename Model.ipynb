{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8be9Ep_V6cv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9LkiPABV9V8",
        "outputId": "3dbc58bb-667b-46e4-f2fd-f5bf1d23922b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "No image files found in the extracted folder.\n"
          ]
        }
      ],
      "source": [
        "!pip install pillow  # Ensure Pillow is installed for image handling\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image  # Import Pillow for image operations\n",
        "\n",
        "# Assuming \"archive (5).zip\" is uploaded in the Colab root directory\n",
        "zip_file_path = \"archive (5).zip\"\n",
        "extraction_path = \"fracture_images\"  # Directory for extracted images\n",
        "\n",
        "# Extract images from the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extraction_path)\n",
        "\n",
        "# Read and display a sample image\n",
        "image_files = [f for f in os.listdir(extraction_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "if image_files:  # Check if any image files were found\n",
        "    sample_image_path = os.path.join(extraction_path, image_files[0])\n",
        "    img = Image.open(sample_image_path)\n",
        "    img.show()  # Display the image\n",
        "    print(\"ok\")\n",
        "else:\n",
        "    print(\"No image files found in the extracted folder.\")\n",
        "\n",
        "# Further processing of images\n",
        "# ... (Your code to load, manipulate, and use the extracted images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XufKCeLxXBqy",
        "outputId": "8dceb4e0-d8d4-4c72-d9b4-31a6fef2fbda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Found 6638 images belonging to 2 classes.\n",
            "Found 1658 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "!pip install pillow  # Ensure Pillow is installed\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# --- Image Extraction ---\n",
        "zip_file_path = \"archive (5).zip\"  # Assuming it's in the Colab root\n",
        "extraction_path = \"fracture_images\"\n",
        "\n",
        "# Extract images (if not already extracted)\n",
        "if not os.path.exists(extraction_path):\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extraction_path)\n",
        "    print(\"Images extracted to:\", extraction_path)\n",
        "\n",
        "# --- Data Loading ---\n",
        "base_dir = extraction_path  # Update base_dir to the extraction path\n",
        "img_size = (128, 128)\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)\n",
        "\n",
        "train_data = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=16,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_data = datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# --- Rest of your code (model building, training, etc.) ---\n",
        "# ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AhZ3vW6XtVN",
        "outputId": "6b71e890-8b00-4989-e683-799ba3025e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),  # Increased units for better representation\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2Rva_oaXvVB",
        "outputId": "2331d825-666a-4ee9-8190-dbe40449600f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 697ms/step - accuracy: 0.4914 - loss: 0.6934 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
            "Epoch 2/3\n",
            "\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 675ms/step - accuracy: 0.5050 - loss: 0.6931 - val_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Epoch 3/3\n",
            "\u001b[1m264/415\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 660ms/step - accuracy: 0.4903 - loss: 0.6932"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=3,  # Increased epochs to 25 (adjust as needed)\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_fracture(image_path, model, img_size=(128, 128)):\n",
        "    img = load_img(image_path, target_size=img_size)\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    prediction = model.predict(img_array)\n",
        "    return 'Fractured' if prediction[0][0] > 0.5 else 'Non-Fractured'\n"
      ],
      "metadata": {
        "id": "20LrzVysZSa7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0VAfSGCqZSCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_fracture('/content/broken-hand-xray.webp', model, img_size=(128, 128)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "052911jlYmjL",
        "outputId": "ecb0d5c9-c78d-4753-9207-866720486fb6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Non-Fractured\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}